{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "import re\n",
    "\n",
    "class EmotionClassifier:\n",
    "    def __init__(self, model_path=\"monologg/kobert\", num_labels=7, device=None):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path,trust_remote_code = True) # trust_remote_code = ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ëŒ€í•œ ê²€ì¦ ì ˆì°¨ê°€ ìƒëµ\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # ê°ì •ë§¤í•‘\n",
    "        self.label_to_emotion = {\n",
    "            0: \"ì¤‘ë¦½\",\n",
    "            1: \"ë†€ëŒ\",\n",
    "            2: \"ë¶„ë…¸\",\n",
    "            3: \"ìŠ¬í””\",\n",
    "            4: \"í–‰ë³µ\",\n",
    "            5: \"í˜ì˜¤\",\n",
    "            6: \"ê³µí¬\"\n",
    "        }\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        \"\"\"í•™ìŠµëœ ê°ì •ë¶„ë¥˜ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\"\"\"\n",
    "        self.model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        return re.sub(\"[^0-9a-zA-Zê°€-í£\\s+]\", \"\", text)\n",
    "\n",
    "    def predict_emotion(self, text):\n",
    "        \"\"\"ê°ì • ë¶„ë¥˜ ë° ì˜ˆì¸¡\"\"\"\n",
    "        # í…ìŠ¤íŠ¸ ì…ë ¥ / í† í°í™” / ë¶„ë¥˜\n",
    "        cleaned_text = self.preprocess_text(text)\n",
    "        encoded_input = self.tokenizer(cleaned_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "        encoded_input = {key: val.to(self.device) for key, val in encoded_input.items()}\n",
    "        \n",
    "        # ì˜ˆì¸¡í•˜ê¸°\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded_input)\n",
    "            predicted_label = outputs.logits.argmax(dim=1).item()\n",
    "        \n",
    "        # ë¶„ë¥˜ëœ ê°ì • ë¼ë²¨\n",
    "        predicted_emotion = self.label_to_emotion[predicted_label]\n",
    "        \n",
    "        return predicted_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display, Markdown\n",
    "import textwrap\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from config.api_keys import gemini_key\n",
    "\n",
    "# Gemini APIì™€ í†µí•©\n",
    "class GeminiService:\n",
    "    def __init__(self, api_key=gemini_key):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    def generate_response(self, emotion, user_text):\n",
    "        \"\"\"\n",
    "        Gemini APIë¥¼ í†µí•´ ê°ì • ê¸°ë°˜ ë‹µë³€ ìƒì„±\n",
    "        \"\"\"\n",
    "        prompt = f\"ë„ˆëŠ” ì‚¬ìš©ìê°€ ì¼ê¸°ë¥¼ ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ê³ , ìœ ë„í•˜ëŠ” ì‚¬ëŒì´ì•¼. í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ ìì„¸í•˜ê²Œ ë¬¼ì–´ë³´ì§€ ë§ê³  ì§§ê²Œ í•œì¤„ì”© ìœ ë„í•´ì¤˜. ì‚¬ìš©ìì˜ ê°ì •ìƒíƒœëŠ” {emotion} ì´ê³ ,  ì‚¬ìš©ìì˜ ë§: {user_text}\"\n",
    "        \n",
    "        # ë§Œë“  í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì‘ë‹µ ìƒì„±\n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        # ì‘ë‹µ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "        return response.text\n",
    "\n",
    "\n",
    "# í†µí•© ì„œë¹„ìŠ¤ í´ë˜ìŠ¤\n",
    "class DiaryService:\n",
    "    def __init__(self, emotion_model_path, gemini_api_key):\n",
    "        \"\"\"\n",
    "        DiaryService ì´ˆê¸°í™”\n",
    "        :param emotion_model_path: KoBERT ê°ì • ë¶„ë¥˜ ëª¨ë¸ ê²½ë¡œ\n",
    "        :param gemini_api_key: Gemini API í‚¤\n",
    "        \"\"\"\n",
    "        # KoBERT ê°ì • ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        self.emotion_classifier = EmotionClassifier()\n",
    "        self.emotion_classifier.load_model(emotion_model_path)\n",
    "\n",
    "        # Gemini API í‚¤ ì €ì¥\n",
    "        self.gemini_api_key = gemini_api_key\n",
    "\n",
    "    def process_input(self, user_text):\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±\n",
    "        :param user_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸\n",
    "        :return: ê°ì • ì˜ˆì¸¡ ê²°ê³¼ ë° Gemini ì‘ë‹µ\n",
    "        \"\"\"\n",
    "        # ê°ì • ì˜ˆì¸¡\n",
    "        predicted_emotion = self.emotion_classifier.predict_emotion(user_text)\n",
    "\n",
    "        # Gemini API í˜¸ì¶œ (ì˜ˆì œ)\n",
    "        response = self.get_gemini_response(user_text)\n",
    "\n",
    "        return {\n",
    "            \"user_text\": user_text,\n",
    "            \"predicted_emotion\": predicted_emotion,\n",
    "            \"response\": response,\n",
    "        }\n",
    "\n",
    "    def get_gemini_response(self, text):\n",
    "        \"\"\"\n",
    "        Gemini API í˜¸ì¶œ (ì˜ˆì œ)\n",
    "        :param text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸\n",
    "        :return: Gemini ì‘ë‹µ í…ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ì—¬ê¸°ì—ì„œ ì‹¤ì œ Gemini API í˜¸ì¶œ ë¡œì§ì„ êµ¬í˜„í•˜ì„¸ìš”.\n",
    "        # ì˜ˆì œì—ì„œëŠ” ê°„ë‹¨íˆ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        return f\"Gemini ì‘ë‹µ: {text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ê²°ê³¼ ---\n",
      "ì…ë ¥ í…ìŠ¤íŠ¸: ë°°ê³ íŒŒ\n",
      "ì˜ˆì¸¡ëœ ê°ì •: ìŠ¬í””\n",
      "Gemini ì‘ë‹µ: Gemini ì‘ë‹µ: ë°°ê³ íŒŒ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from config.api_keys import gemini_key\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # í•™ìŠµëœ KoBERT ëª¨ë¸ íŒŒì¼ ê²½ë¡œì™€ Google Gemini API í‚¤ ì„¤ì •\n",
    "    emotion_model_path = \"new_data_test.pth\"\n",
    "\n",
    "    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”\n",
    "    diary_service = DiaryService(emotion_model_path=emotion_model_path, gemini_api_key=gemini_key)\n",
    "\n",
    "    # ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "    user_input = input(\"í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    result = diary_service.process_input(user_input)\n",
    "\n",
    "    print(\"\\n--- ê²°ê³¼ ---\")\n",
    "    print(f\"ì…ë ¥ í…ìŠ¤íŠ¸: {result['user_text']}\")\n",
    "    print(f\"ì˜ˆì¸¡ëœ ê°ì •: {result['predicted_emotion']}\")\n",
    "    print(f\"Gemini ì‘ë‹µ: {result['response']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ìš©ì´ì˜ ì¼ê¸° ë„ìš°ë¯¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! 'ê·¸ë§Œ'ì´ë¼ê³  ì…ë ¥í•˜ë©´ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ìš”ì•½ëœ ì¼ê¸°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "\n",
      "ì¸ìš©ì´: ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‹œì‘í•˜ë©° ì–´ë–¤ ì¢‹ì€ ì¼ì´ ìˆì—ˆë‚˜ìš”?\n",
      "\n",
      "ì¸ìš©ì´: ì˜¤ëŠ˜ í•˜ë£¨ ì¤‘ ê°€ì¥ ì¸ìƒ ê¹Šì—ˆë˜ ìˆœê°„ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?\n",
      "\n",
      "ì¸ìš©ì´: ì˜¤ëŠ˜ í•˜ë£¨, ì–´ë–¤ ìƒ‰ê¹”ë¡œ ì±„ìƒ‰ë˜ì–´ ìˆë‚˜ìš”?\n",
      "\n",
      "ì¸ìš©ì´: ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì´ˆë¡ìƒ‰ìœ¼ë¡œ í‘œí˜„í•œë‹¤ë©´ ì–´ë–¤ ì¥ë©´ì´ ë– ì˜¤ë¥´ë‚˜ìš”?\n",
      "\n",
      "ì¸ìš©ì´: ì˜¤ëŠ˜ í•˜ë£¨, ëª¸ì„ ë¬´ê²ê²Œ ì§“ëˆ„ë¥´ëŠ” ê°ì •ì€ ë¬´ì—‡ì´ì—ˆë‚˜ìš”?\n",
      "\n",
      "ì¸ìš©ì´: ì˜¤ëŠ˜ ë„ˆë¥¼ ë†€ë¼ê²Œ í•œ ê²ƒì€ ë¬´ì—‡ì´ì—ˆì–´?\n",
      "\n",
      "ì¸ìš©ì´: ğŸ¤¯ ì§€ê¸ˆ ê°€ì¥ ì§œì¦ë‚˜ëŠ” ê±´ ë­ì˜€ì–´?\n",
      "\n",
      "\n",
      "ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ìš”ì•½ëœ ì¼ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\n",
      "\n",
      "### ì¼ê¸° ì‘ì„± (2025-02-28 14:21:05)\n",
      "\n",
      "## 2024ë…„ 5ì›” 15ì¼ (ë‚ ì”¨: íë¦¼)\n",
      "\n",
      "ì˜¤ëŠ˜ í•˜ë£¨ëŠ” í½í½í•œ ë‹­ê°€ìŠ´ì‚´ë¡œ ì‹œì‘í–ˆë‹¤. ë§ˆì¹˜ ë‹­ê°€ìŠ´ì‚´ì˜ í½í½í•¨ì´ ì˜¤ëŠ˜ í•˜ë£¨ì˜ ì„œë§‰ì„ ì•Œë¦¬ëŠ” ë“¯í–ˆë‹¤. ì¸ìš©ì´ë¼ëŠ” ë…€ì„ì€ ì•„ì¹¨ë¶€í„° ëŠì„ì—†ì´ ì§ˆë¬¸ë§Œ ìŸì•„ëƒˆë‹¤. ì¢‹ì€ ì¼ì´ ë­ì˜€ëƒ, ì¸ìƒ ê¹Šì—ˆë˜ ìˆœê°„ì€ ë­ëƒ, ì˜¤ëŠ˜ì€ ë¬´ìŠ¨ ìƒ‰ê¹”ì´ëƒ... ë§ˆì¹˜ ì‹¬ë¦¬ ìƒë‹´ì´ë¼ë„ ë°›ëŠ” ê¸°ë¶„ì´ì—ˆë‹¤.\n",
      "\n",
      "ë‹­ê°€ìŠ´ì‚´ì„ ë¨¹ê³  ë‚˜ë‹ˆ ì‹ê³¤ì¦ì´ ëª°ë ¤ì™”ë‹¤. ì˜¨ëª¸ì´ ë¬´ê²ê²Œ ëŠê»´ì¡Œë‹¤. ì¸ìš©ì´ëŠ” ë˜ë‹¤ì‹œ ì§“ëˆ„ë¥´ëŠ” ê°ì •ì´ ë­ëƒê³  ë¬¼ì–´ë´¤ë‹¤. ì†”ì§íˆ ë§í•´ì„œ, ì˜¤ëŠ˜ ë‚˜ë¥¼ ì§“ëˆ„ë¥´ëŠ” ê±´ ë°”ë¡œ ë„ˆ, ì¸ìš©ì´ ë„ˆë‹¤!\n",
      "\n",
      "ê³„ì†ë˜ëŠ” ì§ˆë¬¸ ê³µì„¸ì— ì§œì¦ì´ ì†Ÿêµ¬ì³¤ë‹¤. ğŸ¤¯ ë„ëŒ€ì²´ ì™œ ì´ë ‡ê²Œ ì§ˆë¬¸ë§Œ í•´ëŒ€ëŠ” ê±°ì•¼? ì¸ìš©ì´ ë•Œë¬¸ì— ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì˜¨í†µ ì´ˆë¡ìƒ‰ìœ¼ë¡œ ë¬¼ë“  ê²ƒ ê°™ë‹¤. ì™ ì§€ ëª¨ë¥´ê²Œ ì”ì“¸í•˜ê³  ë‹µë‹µí•œ ì´ˆë¡ìƒ‰. ë¹¨ë¦¬ ì´ ì§ˆë¬¸ ì§€ì˜¥ì—ì„œ ë²—ì–´ë‚˜ê³  ì‹¶ë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from config.api_keys import gemini_key\n",
    "\n",
    "# EmotionClassifier: KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ í´ë˜ìŠ¤\n",
    "class EmotionClassifier:\n",
    "    def __init__(self, model_path=\"monologg/kobert\", num_labels=7, device=None):\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # ê°ì • ë§¤í•‘ (ë¼ë²¨ ë²ˆí˜¸ -> ê°ì • ì´ë¦„)\n",
    "        self.label_to_emotion = {\n",
    "            0: \"ì¤‘ë¦½\",\n",
    "            1: \"ë†€ëŒ\",\n",
    "            2: \"ë¶„ë…¸\",\n",
    "            3: \"ìŠ¬í””\",\n",
    "            4: \"í–‰ë³µ\",\n",
    "            5: \"í˜ì˜¤\",\n",
    "            6: \"ê³µí¬\"\n",
    "        }\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        \"\"\"í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\"\"\"\n",
    "        self.model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "        return re.sub(\"[^0-9a-zA-Zê°€-í£\\\\s+]\", \"\", text)\n",
    "\n",
    "    def predict_emotion(self, text):\n",
    "        \"\"\"ê°ì • ë¶„ë¥˜ ë° ì˜ˆì¸¡\"\"\"\n",
    "        cleaned_text = self.preprocess_text(text)\n",
    "        encoded_input = self.tokenizer(cleaned_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "        encoded_input = {key: val.to(self.device) for key, val in encoded_input.items()}\n",
    "\n",
    "        # ëª¨ë¸ ì˜ˆì¸¡\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded_input)\n",
    "            predicted_label = outputs.logits.argmax(dim=1).item()\n",
    "\n",
    "        # ì˜ˆì¸¡ëœ ê°ì •ì„ ë°˜í™˜\n",
    "        return self.label_to_emotion[predicted_label]\n",
    "\n",
    "\n",
    "# Gemini APIì™€ í†µí•©\n",
    "class GeminiService:\n",
    "    def __init__(self, api_key=gemini_key):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    def generate_response(self, emotion, user_text):\n",
    "        \"\"\"\n",
    "        Gemini APIë¥¼ í†µí•´ ê°ì • ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±\n",
    "        \"\"\"\n",
    "        prompt = f\"ë„ˆëŠ” ì‚¬ìš©ìê°€ ì¼ê¸°ë¥¼ ì‰½ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ê³ , ìœ ë„í•˜ëŠ” ì‚¬ëŒì´ì•¼. í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ ìì„¸í•˜ê²Œ ë¬¼ì–´ë³´ì§€ ë§ê³  ì§§ê²Œ í•œì¤„ì”© ìœ ë„í•´ì¤˜. ì‚¬ìš©ìì˜ ê°ì •ìƒíƒœëŠ” {emotion} ì´ê³ ,  ì‚¬ìš©ìì˜ ë§: {user_text}\"\n",
    "        \n",
    "        # Generate content using Gemini API\n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        # ì‘ë‹µ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "        return response.text\n",
    "\n",
    "    def summarize_conversation(self, conversation_history):\n",
    "        \"\"\"\n",
    "        Gemini APIë¥¼ í†µí•´ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ì¼ê¸° í˜•ì‹ìœ¼ë¡œ ì‘ì„±\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ëŒ€í™”ë‚´ìš©ì„ ì¼ê¸°í˜•ì‹ìœ¼ë¡œ ìš”ì•½:\n",
    "{conversation_history}\n",
    "ì¼ê¸° í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜ì•¼ í•´\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate summary using Gemini API\n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        # ìš”ì•½ëœ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
    "        return response.text\n",
    "\n",
    "\n",
    "# í†µí•© ì„œë¹„ìŠ¤ í´ë˜ìŠ¤\n",
    "class DiaryService:\n",
    "    def __init__(self, emotion_model_path, gemini_api_key):\n",
    "        # KoBERT ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        self.classifier = EmotionClassifier()\n",
    "        self.classifier.load_model(emotion_model_path)\n",
    "\n",
    "        # Gemini API ì´ˆê¸°í™”\n",
    "        self.gemini_service = GeminiService(api_key=gemini_api_key)\n",
    "\n",
    "    def process_input(self, user_text):\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±\n",
    "        \"\"\"\n",
    "        # ê°ì • ì˜ˆì¸¡\n",
    "        predicted_emotion = self.classifier.predict_emotion(user_text)\n",
    "\n",
    "        # Gemini APIë¡œ ì§ˆë¬¸ ìƒì„±\n",
    "        response_text = self.gemini_service.generate_response(predicted_emotion, user_text)\n",
    "\n",
    "        return predicted_emotion, response_text\n",
    "\n",
    "    def summarize_conversation(self, conversation_history):\n",
    "        \"\"\"\n",
    "        ëŒ€í™” ë‚´ìš© ìš”ì•½ ìš”ì²­\n",
    "        \"\"\"\n",
    "        return self.gemini_service.summarize_conversation(conversation_history)\n",
    "\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ (ëŒ€í™” ì¸í„°í˜ì´ìŠ¤)\n",
    "if __name__ == \"__main__\":\n",
    "    # í•™ìŠµëœ KoBERT ëª¨ë¸ íŒŒì¼ ê²½ë¡œì™€ Google Gemini API í‚¤ ì„¤ì •\n",
    "    emotion_model_path = \"new_data_test.pth\"\n",
    "    gemini_api_key = gemini_key\n",
    "\n",
    "    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”\n",
    "    diary_service = DiaryService(emotion_model_path=emotion_model_path, gemini_api_key=gemini_api_key)\n",
    "\n",
    "    print(\"ì¸ìš©ì´ì˜ ì¼ê¸° ë„ìš°ë¯¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! 'ê·¸ë§Œ'ì´ë¼ê³  ì…ë ¥í•˜ë©´ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ìš”ì•½ëœ ì¼ê¸°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\\n\")\n",
    "\n",
    "    conversation_history = \"\"\n",
    "    while True:\n",
    "        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "        user_input = input(\"ì‚¬ìš©ì: \")\n",
    "        \n",
    "        if user_input.strip().lower() == \"ê·¸ë§Œ\":\n",
    "            print(\"\\nëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ìš”ì•½ëœ ì¼ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\\n\")\n",
    "            diary_entry = diary_service.summarize_conversation(conversation_history)\n",
    "            \n",
    "            # í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "            current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            print(f\"### ì¼ê¸° ì‘ì„± ({current_time})\\n\")\n",
    "            print(diary_entry)\n",
    "            break\n",
    "\n",
    "        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±\n",
    "        predicted_emotion, gemini_response = diary_service.process_input(user_input)\n",
    "\n",
    "        # ëŒ€í™” ê¸°ë¡ ì €ì¥\n",
    "        conversation_history += f\"User: {user_input}\\nì¸ìš©ì´: {gemini_response}\\n\"\n",
    "\n",
    "        # AI ì‘ë‹µ ì¶œë ¥\n",
    "        print(f\"ì¸ìš©ì´: {gemini_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
